{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO495nTlGZIrvvI93QdbKkl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Igben-Nehemiah/ML-practice/blob/main/Classical_ML_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimension Reduction and Latent Variable Methods"
      ],
      "metadata": {
        "id": "Sx8pREowUDHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis (PCA)"
      ],
      "metadata": {
        "id": "_l9OD8CBVbDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality reduction for Polymer Manufacturing Process"
      ],
      "metadata": {
        "id": "N4QGhA85oCC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "B4IN5MWQUGgQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch data and separate training data\n",
        "data = pd.read_excel('proc1a.xls', skiprows=1, usecols='C:AI')\n",
        "data_train = data.iloc[0:69,]"
      ],
      "metadata": {
        "id": "x3lcjG5koh61"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalise data\n",
        "scaler = StandardScaler()\n",
        "data_train_normal = scaler.fit_transform(data_train)"
      ],
      "metadata": {
        "id": "af8FOXMIoyYM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "pca = PCA()\n",
        "score_train = pca.fit_transform(data_train_normal)"
      ],
      "metadata": {
        "id": "uYLfAk9pozrA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm no correlation\n",
        "corr_coef = np.corrcoef(score_train, rowvar=False)"
      ],
      "metadata": {
        "id": "FRuhgIZCqi3e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise explained variance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "explained_variance = 100*pca.explained_variance_ratio_ # in percentage\n",
        "cum_explained_variance = np.cumsum(explained_variance) # cumulative % variance explained"
      ],
      "metadata": {
        "id": "-Tf2bh10rLUO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cum_explained_variance, 'r+', label = 'cumulative % variance explained')\n",
        "plt.plot(explained_variance, 'b+', label = 'variance explained by each PC')\n",
        "plt.ylabel('Explained variance (in %)')\n",
        "plt.xlabel('Principal component number')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "eXpBW7VKr9ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decide number of PCs to retain and compute reduced data in PC space\n",
        "n_comp = np.argmax(cum_explained_variance >= 90) + 1\n",
        "score_train_reduced = score_train[:, 0:n_comp]"
      ],
      "metadata": {
        "id": "8uPgd9lWtvQK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_train_reduced.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIJO0EDb0qw3",
        "outputId": "ca732c9c-9f42-48ae-cfa0-d02a1abf1b72"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm that only about 10% of original information is lost\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "V_matrix = pca.components_.T\n",
        "P_matrix = V_matrix[:, 0:n_comp]\n",
        "\n",
        "data_train_normal_reconstruct = np.dot(score_train_reduced, P_matrix.T)\n",
        "R2_score = r2_score(data_train_normal, data_train_normal_reconstruct)\n",
        "\n",
        "print('% information lost = ', 100*(1-R2_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGhL26CK0tIF",
        "outputId": "3f7b4101-1130-466b-a095-ccb50f9ee752"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% information lost =  9.046972754471994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEYJcole1rWh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}